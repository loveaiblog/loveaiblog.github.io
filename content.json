{"meta":{"title":"Zihan's Blog","subtitle":"","description":"Keep a log of development and learning activities.","author":"Zihan Liu","url":"https://loveaiblog.github.io","root":"/"},"pages":[{"title":"categories","date":"2025-03-01T17:15:21.000Z","updated":"2025-03-01T17:15:21.285Z","comments":true,"path":"categories/index.html","permalink":"https://loveaiblog.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Visualized SAM2 Video Segmentation with Gradio","slug":"sam2gradio","date":"2025-03-05T11:20:00.000Z","updated":"2025-03-05T12:22:42.570Z","comments":true,"path":"2025/03/05/sam2gradio/","permalink":"https://loveaiblog.github.io/2025/03/05/sam2gradio/","excerpt":"","text":"Visualized SAM2 Video Segmentation with GradioThis blog explains a simple implementation of a Gradio-based UI for the SAM2 (Segment Anything 2) video segmentation model. The code processes input videos, performs object segmentation using SAM2, and visualizes results. Let’s break down the key components: Imports &amp; Device Configuration12345678910import gradio as grimport cv2, os, numpy as npimport torch# ... other imports ...# Device selectionif torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;)else: device = torch.device(&quot;cpu&quot;) Functionality: Imports necessary libraries for computer vision, deep learning, and UI creation Automatically selects the best available computation device (CUDA GPU &gt; CPU) Video Processing Utilities1234567def video2images(video, output_folder): # Video to frame extraction logic # ... (skipped for brevity) ...def images2video(image_folder, output_video_path, fps=24): # Frame to video conversion logic # ... (skipped for brevity) ... Key Features: video2images(): Extracts frames from video, skips first frame (assumed to be overlay grid) images2video(): Reconstructs video from processed frames with specified FPS SAM2 Inference PipelineLet’s take a closer look at the core SAM2 inference implementation that powers the video segmentation. This section contains the most critical logic for processing user inputs and generating segmentation masks. Model Initialization12345678910111213141516171819def sam2_inference(images_path, point_x, point_y, save_dir_name): # Model configuration model_cfg = &quot;configs/sam2.1/sam2.1_hiera_l.yaml&quot; sam2_checkpoint = &quot;/path/to/checkpoint.pt&quot; # Handle different compute architectures if device.type == &quot;cuda&quot;: torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16).__enter__() if torch.cuda.get_device_properties(0).major &gt;= 8: torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cudnn.allow_tf32 = True # Initialize predictor with proper device handling from sam2.build_sam import build_sam2_video_predictor predictor = build_sam2_video_predictor( model_cfg, sam2_checkpoint, device=device ) Key Components: Loads model configuration from YAML file Initializes SAM2 predictor with pretrained weights Prompt Processing &amp; State Initialization1234567891011121314151617181920# Initialize inference state with first video frameinference_state = predictor.init_state(video_path=images_path)predictor.reset_state(inference_state)# Process user click coordinatesann_frame_idx = 0 # First frame indexann_obj_id = 1 # First object ID# Convert user click to model inputpoints = np.array([[int(point_x), int(point_y)]], dtype=np.float32)labels = np.array([1], np.int32) # 1 = positive prompt# Add prompts to predictor_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box( inference_state=inference_state, frame_idx=ann_frame_idx, obj_id=ann_obj_id, points=points, labels=labels,) Critical Operations: Load the images from the input video Creates initial inference state from video frames Add user click coordinates to a point prompt Temporal Mask Propagation1234567# Perform video-level mask inferencevideo_segments = &#123;&#125;for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state): video_segments[out_frame_idx] = &#123; out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids) &#125; Core Algorithm: Iterates through video frames sequentially Mask Visualization &amp; Output123456789101112131415161718192021def coloring_mask(mask, color): # Give a mask a color # ... (skipped for brevity) ...# Iterate output masksfor out_frame_idx in range(0, len(frame_names), vis_frame_stride): frame_idx = cv2.imread(...) frame_idx = torch.tensor(frame_idx, dtype=torch.float32).to(device) mask_idx = torch.zeros_like(frame_idx) for out_obj_id, out_mask in video_segments[out_frame_idx].items(): # Apply color blending to masks colored_mask = coloring_mask(out_mask,color[out_obj_id-1]) frame_copy = frame_idx.clone() frame_copy[mask] = colored_mask[mask] mask_idx[mask] = 255 alpha = 0.7 beta = 1 - alpha frame_idx = torch.add(frame_copy * alpha, frame_idx * beta) # Save to path frame_idx = cv2.imwrite(..., frame_idx.cpu().numpy()) mask_idx = cv2.imwrite(..., mask_idx.cpu().numpy()) Visualization Techniques: Implements blending (70% mask opacity &amp; 30% frame opacity) Saves both blended visualizations and raw masks Main Function sam2_video()123456789101112131415161718def sam2_video(video, point_x, point_y): # Convert video to images video2image_saved = ... output_video_folder = ... fps, video_name = video2images(video, video2image_saved) images_path = os.path.join(video2image_saved, video_name) # Perform SAM2 inference images_outputs_path, mask_dir = sam2_inference(images_path, height, width, video_name) # Convert images back to video output_video_path = os.path.join(output_video_folder, video_name + &#x27;.mp4&#x27;) output_mask_path = os.path.join(output_video_folder, video_name + &#x27;_mask.mp4&#x27;) images2video(images_outputs_path, output_video_path, fps) images2video(mask_dir, output_mask_path, fps) return output_video_path, output_mask_path Explanation: It first converts the video into individual images. Then, it runs SAM2 segmentation on those images. Finally, it converts the processed frames back into a video, both with and without masks, and returns the video paths for download. Gradio Interface12345678910111213141516171819202122if __name__ == &quot;__main__&quot;: with gr.Blocks() as demo: with gr.Row(): input_video = gr.Video(format=&#x27;mp4&#x27;,label=&#x27;Source Video&#x27;) first_img = gr.Image(label=&quot;First Image&quot;) with gr.Row(): input_x_cord = gr.Textbox(label=&quot;X Cord&quot;) input_y_cord = gr.Textbox(label=&quot;Y Cord&quot;) with gr.Row(): text_button = gr.Button(&quot;Submit&quot;) with gr.Row(): output_video = gr.Video(format=&#x27;mp4&#x27;,label=&quot;SAM2 Vis&quot;,show_download_button=True) output_mask = gr.Video(format=&#x27;mp4&#x27;,label=&quot;Mask&quot;,show_download_button=True) input_video.upload(load_first_img, [input_video],[first_img]) first_img.select(get_select_coords, [first_img,input_video], [input_x_cord, input_y_cord]) text_button.click(sam2_video, [input_video, input_x_cord, input_y_cord], [output_video, output_mask]) demo.launch() Architecture Overview123456789101112131415graph TD style A fill:#e3f2fd,stroke:#fff,stroke-width:2px style B fill:#c5e1f7,stroke:#fff,stroke-width:2px style C fill:#bbdefb,stroke:#fff,stroke-width:2px style D fill:#90caf9,stroke:#fff,stroke-width:2px style E fill:#81d4fa,stroke:#fff,stroke-width:2px style F fill:#64b5f6,stroke:#fff,stroke-width:2px style G fill:#42a5f5,stroke:#fff,stroke-width:2px A[**Load Video**] --&gt; B[**Get First Frame**] B --&gt; C[**User Click**] C --&gt; D[**Get Point Prompt**] D --&gt; E[**Propagate on Video**] E --&gt; F[**Mask Visulization**] F --&gt; G[**Generate Output Video**] Follow-up DevelopmentThis blog demonstrates the implementation of SAM2 with a single point prompt. You can extend it to multiple prompts as needed (by utilizing gr.render to input a custom number of prompts). In addition, event listeners can be implemented through gr.State that enable to control the frames displayed on the UI through buttons to facilitate the selection of prompts on multiple frames. A more practical and stable implementation of SAM2 involves using a mask as the prompt. To achieve this you need to use predictor.predict() to generate a mask from a point or a box prompt using SAM (see SAM2 Official Tutorial: image_predictor_example.ipynb), which can then be used to invoke SAM2 with the mask prompt for further inference (mask prompt see SAM2 Official Tutorial: automatic_mask_generator_example.ipynb).","categories":[],"tags":[{"name":"Development","slug":"Development","permalink":"https://loveaiblog.github.io/tags/Development/"}]},{"title":"Convert Mask from Numpy Array to RLE (Run-Length Encoding) for CVAT","slug":"mask2rle","date":"2025-03-04T13:27:00.000Z","updated":"2025-03-05T12:22:43.783Z","comments":true,"path":"2025/03/04/mask2rle/","permalink":"https://loveaiblog.github.io/2025/03/04/mask2rle/","excerpt":"","text":"Convert Mask from Numpy Array to RLE (Run-Length Encoding) for CVATWhat is RLE?When reading a mask in the format of numpy.array line by line, you often encounter long sequences of consecutive 0 (or False) followed by sequences of 1 (or True). Based on this observation, we can first define a bounding box that encloses the mask. Then, we read the values line by line within this box and record the counts of consecutive values. For example, consider the following mask enclosed in a box: 12[[0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]] Reading this mask line by line, we get the sequence: 3*0, 6*1, 5*0, 7*1, 3*0. This sequence can be efficiently encoded as the RLE string: &#39;3,6,5,7,3&#39;. If the mask starts with a 1 instead of a 0, we prepend a 0 to the RLE string to indicate that there are zero 0s at the beginning. This is the standard rule for converting a mask to RLE format. Standard Implementation12345678910111213141516171819202122232425def binary_image_mask_to_cvat_rle(mask: np.ndarray) -&gt; dict: # Get box information yy, xx = np.nonzero(mask) top, left = np.min(yy), np.min(xx) bottom, right = np.max(yy), np.max(xx) height, width = bottom - top + 1, right - left + 1 rle = [] offset = 0 # How many consecutive &#x27;0&#x27;s or &#x27;1&#x27;s value = 0 # Are we counting &#x27;0&#x27; or &#x27;1&#x27; # Read line by line for y in range(top, top + height): for x in range(left, left + width): if mask[y][x] == value: offset += 1 else: rle.append(offset) # Save to RLE offset = 1 # Reset &#x27;offset&#x27; value = 1 - value # Flip &#x27;value&#x27; if offset &gt; 0: rle.append(offset) return &#123;&quot;rle&quot;: rle, &quot;left&quot;: left, &quot;top&quot;: top, &quot;width&quot;: width, &quot;height&quot;: height&#125; The code requires iterating over the entire box, resulting in a loop that runs height * width times. Converting a single mask to RLE takes about 0.02 seconds on my device. When processing masks in batches, you might need to wait for tens of seconds in the CVAT interface just for format conversion. We can replace the above method with a parallelizable approach. Time-efficient ImplementationFirst, we need to determine which positions in the mask transition from 0 to 1 and which positions transition from 1 to 0. 12345bbox_mask = mask[top:bottom + 1, left:right + 1]flat_mask = bbox_mask.flatten() # transform the mask to a 1D arraydiff = np.diff(flat_mask, prepend=0) # compute the differenceschanges = np.where(np.isin(diff, [1, -1]))[0] # find where the value changes Due to the use of prepend=0, changes=0 if flat_mask[0]=1. This is great because the first element in changes will always stands for the number of 0s in the mask. Except for the first element, which represents the count of 0s, the difference between each pair of adjacent elements in changes represents the count of consecutive 0s or 1s. The difference between the last element and the length of flat_mask is the last number to be added to the RLE. 12345rle = []rle.append(changes[0])for i in range(len(changes)-1): rle.append(changes[i+1] - changes[i])rle.append(len(flat_mask)-changes[-1]) The time-efficient implementation: 12345678910111213141516171819202122232425def binary_image_mask_to_cvat_rle(mask: np.ndarray) -&gt; dict: &quot;&quot;&quot; Optimized version of converting binary image mask to CVAT RLE. &quot;&quot;&quot; yy, xx = np.nonzero(mask) if len(yy) == 0: return &#123;&quot;size&quot;: mask.shape, &quot;counts&quot;: []&#125; top, left = np.min(yy), np.min(xx) bottom, right = np.max(yy), np.max(xx) height, width = bottom - top + 1, right - left + 1 bbox_mask = mask[top:bottom + 1, left:right + 1] flat_mask = bbox_mask.flatten() diff = np.diff(flat_mask, prepend=0) changes = np.where(np.isin(diff, [1, -1]))[0] rle = [] rle.append(changes[0]) for i in range(len(changes)-1): rle.append(changes[i+1] - changes[i]) rle.append(len(flat_mask)-changes[-1]) return &#123;&quot;rle&quot;: rle, &quot;left&quot;: left, &quot;top&quot;: top, &quot;width&quot;: width, &quot;height&quot;: height&#125;","categories":[],"tags":[{"name":"Development","slug":"Development","permalink":"https://loveaiblog.github.io/tags/Development/"}]},{"title":"Designing UI with Gradio","slug":"Designing-UI-with-Gradio","date":"2025-03-02T11:18:00.000Z","updated":"2025-03-05T13:37:00.589Z","comments":true,"path":"2025/03/02/Designing-UI-with-Gradio/","permalink":"https://loveaiblog.github.io/2025/03/02/Designing-UI-with-Gradio/","excerpt":"","text":"Designing UI with GradioQuick Start with gr.InterfaceLet’s start from a simplest demo. 1234567import gradio as grdef your_function(input_text): return &quot;For example, a translation model&#x27;s output taking&quot; + input_text + &quot;as input&quot;demo = gr.Interface(fn=your_function, inputs=&quot;text&quot;, outputs=&quot;text&quot;)demo.launch() gr.Interface allows us to define a UI for your_function. The type of inputs and outputs must be predefined. Gradio allows &#39;text&#39; &#39;image&#39; &#39;number&#39; &#39;audio&#39; &#39;video&#39; for both inputs and outputs. Other types that might be useful:Input-only: &#39;slider&#39; - inputting values within a specified range using a slider &#39;checkbox&#39; - boolean input &#39;radio&#39; - single selection input &#39;dropdown&#39; - selecting from a dropdown menu Output-only: &#39;label&#39; - displaying labels or classification results &#39;plot&#39; - displaying charts (e.g., Matplotlib or Plotly plots) Multiple inputs and outputs by putting types into a list, for example: 1234567import gradio as grdef your_function(input_text, language): return &quot;A&quot; + language + &quot;translation model&#x27;s output taking&quot; + input_text + &quot;as input&quot;demo = gr.Interface(fn=your_function, inputs=[&quot;text&quot;,&quot;text&quot;], outputs=&quot;text&quot;)demo.launch() After running the python script, we can Ctrl + Click http://localhost:7860 from your terminal, then you can see a interface in our browser like this: Does the layout look a bit different? Well, the basic functionality is the same as in the previous example. In this case, we define a gr.Blocks, which is like taking a sheet of A4 paper when you start working on an assignment. Then we create two gr.Textbox and two gr.Button on the paper, and they are automatically arranged within this block area. LayoutYou can design your layout using gr.Column and gr.Row to make it look like the UIs you see on Hugging Face. For example: 12345678910111213141516171819import gradio as grwith gr.Blocks() as demo: with gr.Row(): text1 = gr.Textbox(label=&quot;t1&quot;) slider2 = gr.Textbox(label=&quot;s2&quot;) drop3 = gr.Dropdown([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], label=&quot;d3&quot;) with gr.Row(): with gr.Column(scale=1, min_width=300): text1 = gr.Textbox(label=&quot;prompt 1&quot;) text2 = gr.Textbox(label=&quot;prompt 2&quot;) inbtw = gr.Button(&quot;Between&quot;) text4 = gr.Textbox(label=&quot;prompt 1&quot;) text5 = gr.Textbox(label=&quot;prompt 2&quot;) with gr.Column(scale=2, min_width=300): img1 = gr.Image(&quot;images/cheetah.jpg&quot;) btn = gr.Button(&quot;Go&quot;)demo.launch() Like the following code, you can divide gr.Blocks into multiple horizontal sub-areas, and then further divide one of these sub-areas into multiple vertical sub-areas. 12345678with gr.Blocks() as demo: with gr.Row(): ... with gr.Row(): with gr.Column(): ... with gr.Column(): ... To limit the shape of a block, try: 1with gr.Column(scale=2, min_width=300) Function Trigger MethodsAlthough my favorite is Button, there are other ways to trigger your function. You can try using an event listener or triggering the function by pressing ENTER. The former can monitor changes in the input box in real-time, while the latter replaces the button with a keyboard key. 1234567891011# Button btn = gr.Button(&quot;Submit&quot;)submit_btn.click(fn=your_function, inputs=input, outputs=output) # Event listener inp = gr.Textbox()inp.change(welcome, inp, out)# &#x27;ENTER&#x27; inp = gr.Textbox(label=&quot;Input&quot;)inp.submit(greet, name, [output, trigger]) You can also set multiple triggers using gr.Blocks(). 1gr.on(triggers=[name.submit, greet_btn.click], fn=greet, inputs=name, outputs=output).then(clear_name, outputs=[name]) You may use .then(clear_name, outputs=[name]) to call a second function after ‘greet’ has been triggered. Here, outputs=[name] means that the return value will update the “name” input box. The .then() operation can actually be written inside greet, making greet look cleaner, but it is not mandatory. Controling the visibility of a sub-block123456789101112131415161718192021222324252627import gradio as grwith gr.Blocks() as demo: name_box = gr.Textbox(label=&quot;Name&quot;) age_box = gr.Number(label=&quot;Age&quot;, minimum=0, maximum=100) symptoms_box = gr.CheckboxGroup([&quot;Cough&quot;, &quot;Fever&quot;, &quot;Runny Nose&quot;]) submit_btn = gr.Button(&quot;Submit&quot;) with gr.Column(visible=False) as output_col: diagnosis_box = gr.Textbox(label=&quot;Diagnosis&quot;) patient_summary_box = gr.Textbox(label=&quot;Patient Summary&quot;) def submit(name, age, symptoms): return &#123; submit_btn: gr.Button(visible=False), output_col: gr.Column(visible=True), diagnosis_box: &quot;covid&quot; if &quot;Cough&quot; in symptoms else &quot;flu&quot;, patient_summary_box: f&quot;&#123;name&#125;, &#123;age&#125; y/o&quot;, &#125; submit_btn.click( submit, [name_box, age_box, symptoms_box], [submit_btn, diagnosis_box, patient_summary_box, output_col], )demo.launch() In this example, clicking the submit_btn triggers the submit function. The change in visibility is specified as one of the outputs.You can modify the visibility of individual components or an entire block. If it applies to a block, you need to define a variable for it when creating the block. Determine the Output Boxes Based on InputIf you are unsure how many outputs will be generated before entering your inputs, you can use gr.render(). 1234567891011121314import gradio as grwith gr.Blocks() as demo: input_text = gr.Textbox(label=&quot;Input&quot;) @gr.render(inputs=input_text) def show_split(text): if len(text) == 0: gr.Markdown(&quot;## No Input Provided&quot;) else: for letter in text: gr.Textbox(letter)demo.launch() @gr.render(inputs=input_text) is actually listening to the change in input_text if there is no trigger defined. You can use this listening mechanism to create some interesting designs. For example: 1234567891011121314151617181920212223import gradio as grwith gr.Blocks() as demo: text_count = gr.State(1) add_btn = gr.Button(&quot;Add Box&quot;) add_btn.click(lambda x: x + 1, text_count, text_count) @gr.render(inputs=text_count) def render_count(count): boxes = [] for i in range(count): box = gr.Textbox(key=i, label=f&quot;Box &#123;i&#125;&quot;) boxes.append(box) def merge(*args): return &quot; &quot;.join(args) merge_btn.click(merge, boxes, output) merge_btn = gr.Button(&quot;Merge&quot;) output = gr.Textbox(label=&quot;Merged Output&quot;)demo.launch() Here, gr.State() allows you to listen to events in the state variable. If you are adding a new box through gr.render() but have already input something into the existing boxes, remember to set key= to maintain the content in the displayed boxes. Provide Some Examples on the UIIf you want to show users how to use your UI, you can use gr.Examples() to provide pre-filled example inputs. This helps users understand how to interact with your interface more effectively. 123456789101112131415161718192021222324252627282930313233343536import gradio as grdef calculator(num1, operation, num2): if operation == &quot;add&quot;: return num1 + num2 elif operation == &quot;subtract&quot;: return num1 - num2 elif operation == &quot;multiply&quot;: return num1 * num2 elif operation == &quot;divide&quot;: return num1 / num2with gr.Blocks() as demo: with gr.Row(): with gr.Column(): num_1 = gr.Number(value=4, label=&quot;Number 1&quot;) operation = gr.Radio([&quot;add&quot;, &quot;subtract&quot;, &quot;multiply&quot;, &quot;divide&quot;], label=&quot;Operation&quot;) num_2 = gr.Number(value=0, label=&quot;Number 2&quot;) submit_btn = gr.Button(value=&quot;Calculate&quot;) with gr.Column(): result = gr.Number(label=&quot;Result&quot;) submit_btn.click( calculator, inputs=[num_1, operation, num_2], outputs=[result], api_name=False ) examples = gr.Examples( examples=[ [5, &quot;add&quot;, 3], [4, &quot;divide&quot;, 2], [-4, &quot;multiply&quot;, 2.5], [0, &quot;subtract&quot;, 1.2], ], inputs=[num_1, operation, num_2], label=&quot;Examples&quot; )demo.launch(show_api=False) In this example, gr.Examples() provides a set of predefined input combinations. Users can click on any example to automatically fill the input boxes with those values. This feature makes it easier for users to test different scenarios and understand how the calculator works. Select a Point on the gr.Image or gr.DataframeIf you want to click on an image to get the coordinates of where you clicked relative to the image, you can use gr.select() to achieve this. 12345678910def get_select_coords(img, evt: gr.SelectData): cord_x = evt.index[0] cord_y = evt.index[1] return cord_x, cord_ywith gr.Blocks() as demo: first_img = gr.Image(label=&quot;First Image&quot;) x_cord = gr.Textbox(label=&quot;X Coordinate&quot;) y_cord = gr.Textbox(label=&quot;Y Coordinate&quot;) first_img.select(get_select_coords, [first_img], [x_cord, y_cord]) Similarly, gr.Dataframe is a table-format component that also supports the select method. Summary of Useful Functions for gr.Blocks12345678gr.Row(scale=1, min_width=300) gr.Column() gr.Column(visible=False) # Hide a block unless a trigger change the state of &#x27;visible&#x27; gr.select() # Returns the interaction information when interacting with a variable of a certain type gr.Tab((&quot;Next Page&quot;)) # Select one of the multiple Tabs that the current UI displays. gr.Accordion(&quot;Open for More!&quot;, open=False) # A block that can be hidden gr.render() # Generate boxes according to your inputs gr.Examples() # Display input examples, support clicking examples to fill in the input","categories":[],"tags":[{"name":"Development","slug":"Development","permalink":"https://loveaiblog.github.io/tags/Development/"}]}],"categories":[],"tags":[{"name":"Development","slug":"Development","permalink":"https://loveaiblog.github.io/tags/Development/"}]}