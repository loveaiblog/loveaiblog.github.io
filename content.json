{"meta":{"title":"Zihan's Blog","subtitle":"","description":"A diary of AI development and learning activities.","author":"Zihan Liu","url":"https://loveaiblog.github.io","root":"/"},"pages":[{"title":"categories","date":"2025-03-01T17:15:21.000Z","updated":"2025-03-01T17:15:21.285Z","comments":true,"path":"categories/index.html","permalink":"https://loveaiblog.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Neural Radiance Fields (NeRF) — The Magic of MLP","slug":"nerf","date":"2025-04-01T13:58:31.000Z","updated":"2025-04-01T14:50:24.488Z","comments":true,"path":"2025/04/01/nerf/","permalink":"https://loveaiblog.github.io/2025/04/01/nerf/","excerpt":"","text":"NeRF is one of the most interesting technologies I’ve ever come across. It pulls off both 2D view transformation and 3D reconstruction simultaneously—all with just an MLP. NeRF learns the color and opacity of every coordinate in a scene, conditioned on the viewing angle. Some people say NeRF’s training process is just overfitting. I’d say that’s both true and not quite true. The argument for overfitting comes from the fact that NeRF is designed to fit a specific scene rather than generalizing across multiple ones. But in my view, within the 3D world it has seen, NeRF actually achieves perfect generalization across all possible viewpoints. Data NeRF takes in a five-dimensional input—three dimensions representing the observation position and two for the viewing angle (camera parameters). The output consists of four dimensions: three for color and one for opacity. For the highest quality training data, it's best if you can provide camera parameters with minimal error. But if you don't have access to them, no worries—you can use Colmap to match your images and infer the camera parameters automatically. To prepare training data for NeRF, you need to capture images from different heights while circling around the target object. For example, if you're reconstructing an anime figurine, you should take shots at different levels—around the lower legs, waist, and neck. Additionally, you should capture extra images of hard-to-see areas to prevent texture loss. If you're using Colmap, there’s one extra thing to keep in mind: Colmap relies on distinctive points in the images for matching. These points can be found at high-contrast edges, such as the border of a QR code, a sharp corner of a wall, or any region with prominent textures. With the right image collection strategy, at least 200 images should be collected for a fair-enough NeRF reconstruction. Method NeRF’s network architecture is incredibly simple—just an MLP that maps a 5D input to a 4D output: Anyone unfamiliar with NeRF might assume that the input represents the camera’s viewpoint from the training data, while the output corresponds to each pixel’s RGB values—plus some mysterious σ whose meaning isn’t immediately clear. In reality, the camera viewpoint in the input doesn’t represent an observation angle in the traditional sense. Instead, it should be understood as a set of points along the viewing direction. NeRF models the continuous distribution of color and opacity in the interested 3D space. The observed color at a given viewpoint is then computed as the accumulated contribution of all these color blocks along the viewing direction, weighted by their opacity. This is represented by the equation: where: represents how much light is transmitted past all points before point , calculated as: represents the fraction of light contributed by point , given by: Since many sampled points fall in the air with zero opacity, uniform sampling can lead to inefficient computations. To improve efficiency, NeRF treats as a probability distribution and samples more heavily weighted points to make the data more effective and meaningful. When constructing training data, NeRF extracts the incident angle and RGB value for each pixel from the input images and camera parameters. For every pixel, it samples a series of points along the corresponding ray direction and feeds them into an MLP to predict their RGB colors and opacity values. The final color for that pixel is then computed by integrating the contributions of all sampled points along the ray, and this predicted color is supervised by the ground-truth RGB value from the original image. Since NeRF's input incorporates the viewing angle (incident direction), it enables the same point to exhibit different colors when observed from different perspectives. This allows view-dependent lighting effects to be rendered - such as sunlight reflections on glass surfaces.","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://loveaiblog.github.io/tags/Algorithm/"}]},{"title":"Why Can Grounding DINO Achieve Zero-Shot Object Detection?","slug":"groundingdino","date":"2025-03-25T04:57:38.000Z","updated":"2025-03-25T04:59:49.349Z","comments":true,"path":"2025/03/25/groundingdino/","permalink":"https://loveaiblog.github.io/2025/03/25/groundingdino/","excerpt":"","text":"Grounding DINO has recently gained popularity for integrating text-to-image alignment into a DETR-based object detection (OD) model. While it is neither the first model to introduce grounded object detection nor the first to propose a Transformer-based end-to-end OD approach, it achieves competitive performance by simply combining GLIP and DINO. In this post, I’ll walk you through the design of Grounding DINO and explain why it excels at zero-shot object detection. Model Structure image1 The figure above, taken from the published paper, provides an overview of Grounding DINO, highlighting two key components. Let's start with the input: for a grounded OD model, there are two types of inputs—an image and a text prompt. Grounding DINO supports text prompts in two formats: a full sentence or a set of keywords. Based on my experience, it performs better when using keywords. As stated in the paper, when Grounding DINO detects that the input is a full sentence, it processes the text through a filtering block to extract keywords. However, this means some important information might be lost. For example, if we provide an image and the text prompt \"cat, person, mouse\", the first step is feature extraction using pretrained backbones. The text backbone is BERT, while the image backbone is Swin Transformer. The raw feature dimensions are as follows: Text feature: (sequence length, d) Image feature: (H, W, d) Feature Enhancer Next, these raw features are fed into the feature enhancer, which consists of multiple Feature Enhancer Layers (shown in the bottom right of the figure). Within each layer, there is: Deformable self-attention – a more computationally efficient version of standard self-attention. Image-to-text cross-attention, which takes the tokenized image features as queries (Q) and outputs a feature of shape (H × W, d) (and vice versa for text-to-image). Language-Guided Query Selection The Language-Guided Query Selection module is illustrated in the figure below. Essentially, it selects the top-k image tokens that are most relevant to the text tokens, refining the alignment between text and image features. image2 Cross-Modality Decoder Each layer in the Cross-Modality Decoder, shown in the overview, is a stacked block consisting of: Self-attention Two cross-attention layers A feed-forward network (FFN) Each output token represents a bounding box along with a feature vector, which is then used to ground the detection to the text prompt. Supervision The loss function in Grounding DINO consists of two main components: Localization loss Alignment loss Localization Loss The localization loss is a weighted sum of: L1 loss – Measures the absolute difference between the predicted and ground-truth box positions. GIOU loss – Solve the 0 gradient problem of intersection-over-union (IoU) when the predicted and ground-truth boxes are not intersected. IOU loss can produce gradients for boxes because the intersection and union of bounding boxes are computed using (x, y, w, h), e.g., x - w/2. Alignment Loss The alignment loss is an InfoNCE (contrastive) loss, designed to associate bounding boxes with their corresponding text prompts. It works by: Forming positive sample pairs between bounding boxes and their corresponding class tokens. Forming negative sample pairs between bounding boxes and all other tokens. This alignment loss serves as a classifier for grounding during the inference phase.","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://loveaiblog.github.io/tags/Algorithm/"}]},{"title":"How the Segment Anything Model 2 (SAM2) Works at a High Level","slug":"sam2","date":"2025-03-18T14:02:01.000Z","updated":"2025-03-18T14:21:24.541Z","comments":true,"path":"2025/03/18/sam2/","permalink":"https://loveaiblog.github.io/2025/03/18/sam2/","excerpt":"","text":"With the continued development of Segment Anything Model 2 (SAM2), I believe it's essential to dive into the algorithmic details of SAM2 to gain a deeper understanding of how it works and its potential. In this blog, I'll highlight the most important aspects of SAM2, distilling key insights to help readers grasp its fundamentals and applications. How to use SAM2 The primary function of SAM2 is to extract masklets (i.e., a masklet is a set of masks for the same object) from videos based on interactive prompts. The supported prompt formats include: Point: Formatted as [x, y], with attributes for positive and negative. Box: Formatted as [x_min, y_min, x_max, y_max] without normalization. Mask: Formatted as numpy.array(H, W). The model takes in all frames of a video, the content of the prompt(s), the corresponding frame, and the instance ID as input. It then outputs a dictionary of masks indexed by frame number and prompt instance ID, formatted as: &#123; 0: &#123;&#39;obj_0&#39;:..., &#39;obj_1&#39;:...&#125;, 1: &#123;&#39;obj_0&#39;:..., &#39;obj_1&#39;:...&#125;, ... &#125; According to the official documentation, it may lost tracking during inference, as shown in Step 1, Frames 3 &amp; 4 of the figure. However, thanks to the memory bank, a simple positive point can restore tracking (as demonstrated in Step 2). In practice, this feature isn't particularly useful since it provides only a limited efficiency boost for point-based pipelines while being quite complex to deploy. If deploying on an annotation platform, the best approach is to generate masks using SAM and then process them with SAM2. For developing automated annotation tools, bounding boxes are the most recommended prompt format. Architecture According to the official report, SAM2's multi-object inference is essentially built upon single-frame inference. In other words, during the model’s forward pass, it only considers one prompt at a time. For example, if a prompt marks a cat in the first frame, SAM2 follows these steps for subsequent inference: Image Encoder: Uses MAE-pretrained Hiera (a pyramid feature network) as the image encoder to extract visual information. Memory Attention: Employs a Transformer-based structure to integrate image features with the memory bank. Specifically, it stacks multiple layers of self-attention (for image embeddings) and cross-attention in an interleaved \"sandwich\" pattern. Prompt Encoder: Extracts prompt information in token form, using the same prompt encoder as SAM. Memory Decoder: Performs attention operations on the fused image embeddings, memory-merged outputs, and prompt tokens (see diagram for details). The IoU score selects the highest-scoring mask from multiple outputs. The occlusion score determines whether the target object is absent in the current frame (which explains why SAM2 sometimes loses tracking). The object pointer carries memory information forward for the next frame’s prediction. Memory Encoder: The output masks are downsampled, element-wise summed with the image encoder output, then passed through a convolutional network before being stored in the memory bank. Memory Bank: Stores information from the most recent N frames and permanently retains M frames that contain prompts. Data and Training The development team collected and annotated the SA-V dataset, which consists of 50.9K videos and 190.9K masklets. This dataset was manually labeled using both SAM and SAM2. As the dataset size increased, SAM2's performance improved, and annotation efficiency also benefited from iterative enhancements to SAM2 itself. SAM2 was initially trained on SA-1B, a static image segmentation dataset. Given its architecture, during this phase, memory attention only applied self-attention to image embeddings, while output tokens were ignored in the memory decoder. The loss function used in pretraining differs slightly from that of SAM—details can be found in the research paper. After pretraining, SAM2 was further trained on video data, which included SA-V, internal company-licensed videos, and a subset (10%) of SA-1B. The training alternated between video and static image datasets, with the data volume for each training iteration proportional to the dataset’s size. A few additional key points: During training, the ratio of prompt types was 50% mask, 25% point, and 25% box. Videos had a 50% probability of being flipped to enable bidirectional propagation (which I discuss in another blog post). Random horizontal flips and random affine transforms were used for data augmentation. Conclusion SAM2 represents a significant step forward in video segmentation by integrating memory-based tracking with prompt-guided mask extraction. While it builds on the strengths of SAM, its reliance on single-frame inference and complex deployment considerations mean it is best suited for specific use cases, such as annotation platforms and automated labeling tools. As research in segmentation models continues to evolve, improvements in tracking stability and efficiency will likely make models like SAM2 even more powerful. I hope this post helps clarify how SAM2 works at a high level. If you're interested in exploring more, I encourage you to experiment with SAM2 yourself or dive into the official research paper for deeper insights.","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://loveaiblog.github.io/tags/Algorithm/"}]},{"title":"Autonomous Video Tracking & Segmentation","slug":"auto-video-tracking","date":"2025-03-13T13:50:48.000Z","updated":"2025-03-18T14:05:33.199Z","comments":true,"path":"2025/03/13/auto-video-tracking/","permalink":"https://loveaiblog.github.io/2025/03/13/auto-video-tracking/","excerpt":"","text":"SAM2 (Segment Anything 2) introduces a user-friendly human-computer interaction paradigm, enabling users to generate full video mask trajectories through simple or complex prompts. This innovation dramatically reduces video annotation labor from per-frame instance labeling to single-annotation tracking, significantly compressing time costs for video data preparation. To achieve fully automated annotation, I think developing automatic prompt generation systems for SAM2 worth a shot. SAM2 supports mask generation using points/bounding boxes on specified frames, as well as video inference through points/boxes/mask prompts. Among these three approaches, I typically first obtain initial masks via SAM using point/box prompts, then feed these masks into SAM2 for temporal propagation. When comparing point vs box prompting, bounding boxes prove more automation-friendly. Point-based methods inherently require human-curated positive/negative points, demanding interactive visualization. While box-generated masks may lack precision, automated box generation remains comparatively simpler. For users possessing substantial domain-specific segmentation data, training custom YOLO models for detection/segmentation is advisable. However, in most zero-shot application scenarios, existing generalized models are all I have. Object Detection Models Now let's discuss how to combine detection networks with SAM2 for video inference. Among detection models, I recommend GroundingDINO and Florence-2-large-ft. Notably, Florence-2 offers multi-functional capabilities including object detection, grounding, and caption+grounding. Through empirical testing, I found GroundingDINO outperforms Florence-2 in grounding tasks, thus primarily utilizing Florence-2 for detection. The critical distinction lies in GroundingDINO's requirement for text captions as prompts. Several examples indicate optimal performance when using single-keyword captions formatted as 'keyword1,keyword2,...' - this approach maintains detection stability while allowing multi-instance capture. However, GroundingDINO exhibits high sensitivity to hyperparameter tuning. Excessive box thresholds increase missed detections, while insufficient thresholds boost false positives. Parameter optimization must consider image quality and model generalization - in layman's terms, expect substantial trial-and-error. SAM2 Integration Having established frame-wise bounding box generation capabilities, a basic implementation involves acquiring mask prompts from the first frame and tracking instances throughout the video. Reference implementations include Florence2 + SAM2 and Grounded-SAM-2. These solutions suffice for tracking single persistent instances (e.g., camera-followed objects). However, dynamic scenes requiring detection of emerging objects demand more sophisticated approaches. For comprehensive video instance tracking, I cannot rely solely on initial frame detections. My goal is full automation rather than assisted annotation, eliminating real-time constraints. Here's how I think about it: Require per-frame detection with novel instance identification Generate initial frame masks, then detect new instances in subsequent frames Determine novelty through mask overlap analysis between detections and existing instances Implement quality control to filter SAM's fragmented outputs (familiar users recognize these artifacts) Address detection inconsistencies - new boxes might represent previously missed instances Enable temporal backward propagation since SAM2 lacks native backward inference Implement duplicate prevention through mask overlap checks and quality filtering Thus, the implementation logic unfolds as follows: Frame extraction from video Per-frame object detection Iterative processing from initial frame: Convert current frame bounding boxes to masks via SAM with post-processing Identify novel instances through mask overlap analysis Feed masks to SAM2 for video propagation Reinitialize video predictor with reversed frame other and repropagate for backward propagation Deduplicate trajectories through overlap checks Apply post-processing and update master mask repository Here is the core implementation (using Florence 2 as an example): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160import gradio as grimport numpy as npimport torchfrom transformers import AutoProcessor, AutoModelForCausalLM...# Functions for Florence 2 Inferencemodels = &#123; &#x27;microsoft/Florence-2-large-ft&#x27;: AutoModelForCausalLM.from_pretrained(&#x27;/data/zihan/florence2_sam2/Florence-2-large-ft&#x27;, trust_remote_code=True).to(&quot;cuda&quot;).eval(),&#125;processors = &#123; &#x27;microsoft/Florence-2-large-ft&#x27;: AutoProcessor.from_pretrained(&#x27;/data/zihan/florence2_sam2/Florence-2-large-ft&#x27;, trust_remote_code=True),&#125;@spaces.GPUdef run_example(task_prompt, image, text_input=None, model_id=&#x27;microsoft/Florence-2-large&#x27;): model = models[model_id] processor = processors[model_id] if text_input is None: prompt = task_prompt else: prompt = task_prompt + text_input inputs = processor(text=prompt, images=image, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;) generated_ids = model.generate( input_ids=inputs[&quot;input_ids&quot;], pixel_values=inputs[&quot;pixel_values&quot;], max_new_tokens=1048, early_stopping=False, do_sample=False, num_beams=5, ) generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0] parsed_answer = processor.post_process_generation( generated_text, task=task_prompt, image_size=(image.width, image.height) ) return parsed_answerdef process_image(image, model_id=&#x27;microsoft/Florence-2-large-ft&#x27;): image = Image.fromarray(image) task_prompt = &#x27;&lt;OD&gt;&#x27; results = run_example(task_prompt, image, model_id=model_id) return resultsdef florence_inf(video): cap = cv2.VideoCapture(video) frame_bboxes = [] while(True): ret, frame = cap.read() if not ret: break frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) output_text = process_image(frame) bboxes = output_text[&#x27;&lt;OD&gt;&#x27;][&#x27;bboxes&#x27;] frame_bboxes.append(bboxes) cap.release() return frame_bboxes,&#x27;Florence-2 OD has done its job ξ( ✿＞◡❛)&#x27;def video2images(video, output_folder): # Read the video and save the images to a path # ... (skipped for brevity) ...def crashed(mask): # Check if the masked region is connected # ... (skipped for brevity) ...def overlapped(exist_masks, new_mask): # Check if new mask is overlaped with existing masks # ... (skipped for brevity) ...def fill_small_bubbles(mask): # Find and fill blank region inside a mask # ... (skipped for brevity) ...def median_filter(mask): # ... (skipped for brevity) ...def sam2_inference(images_path, frame_bboxes, save_dir_name, vis_frame_stride=1): # Set checkpoints, device, paths, etc. # ... (skipped for brevity) ... # Initialize SAM2 for image and video predictor from sam2.build_sam import build_sam2_video_predictor, build_sam2 from sam2.sam2_image_predictor import SAM2ImagePredictor video_predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device) inference_state = video_predictor.init_state(video_path=images_path) sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device) image_predictor = SAM2ImagePredictor(sam2_model) video_segments = &#123;&#125; # Saved masks # The loop starts for ann_frame_idx in range(num_frames): video_predictor.reset_state(inference_state) # Get predicted masks from boxes through image predictor img_predictor.set_image(image) masks_all, _, _ = img_predictor.predict( point_coords=None, point_labels=None, box=bboxes, multimask_output=False, ) # post-processing masks = [] if ann_frame_idx in video_segments: for i in range(len(masks_all)): if overlapped(video_segments[ann_frame_idx], masks_all[i]) or crashed(masks_all[i]): continue else: masks.append(masks_all[i]) # Add masks to the video predictor and video propagation # ... (skipped for brevity) ... # Reverse the video inference_state[&#x27;images&#x27;] = torch.flip(inference_state[&#x27;images&#x27;], dims=[0]) predictor.reset_state(inference_state) # Add masks to the video predictor and video propagation # ... (skipped for brevity) ... # Reserse the video again inference_state[&#x27;images&#x27;] = torch.flip(inference_state[&#x27;images&#x27;], dims=[0]) # Post-process all the generated masks and save for out_frame_idx in range(0, len(frame_names), vis_frame_stride): for out_obj_id, out_mask in video_segments[out_frame_idx].items(): # Check if the output mask should be added if crashed(out_mask) or overlapped(exist_masks, out_mask): continue out_mask = median_filter(out_mask) out_mask = fill_small_bubbles(out_mask) # Visualization and write # ... (skipped for brevity) ...def images2video(image_folder, output_video_path, fps=24): # Frame to video conversion logic # ... (skipped for brevity) ...def sam2_video(input_video, florence_bboxes): video2images(...) sam2_inference(...) images2video(...) return output_video, output_maskif __name__ == &quot;__main__&quot;: with gr.Blocks() as demo: gr.Markdown(&quot;# Florence-2 + SAM2&quot;) with gr.Row(): with gr.Column(): input_video = gr.Video(format=&#x27;mp4&#x27;,label=&#x27;Source Video&#x27;) florence_bboxes = gr.State() terminal = gr.Textbox(label=&#x27;Pseudo Terminal&#x27;) with gr.Column(): output_video = gr.Video(format=&#x27;mp4&#x27;,label=&quot;SAM2 Vis&quot;,show_download_button=True) output_mask = gr.Video(format=&#x27;mp4&#x27;,label=&quot;Mask&quot;,show_download_button=True) input_video.upload(florence_inf, [input_video], [florence_bboxes,terminal]) terminal.change(sam2_video, [input_video, florence_bboxes], [output_video, output_mask]) demo.launch(server_port=your_port) Example of Florence 2 + SAM 2 Example of GroundingDINO + SAM 2： To contact me, send an email to zihanliu@hotmail.com.","categories":[],"tags":[{"name":"Development","slug":"Development","permalink":"https://loveaiblog.github.io/tags/Development/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://loveaiblog.github.io/tags/Algorithm/"}]},{"title":"Visualized SAM2 Video Segmentation with Gradio","slug":"sam2gradio","date":"2025-03-05T11:20:00.000Z","updated":"2025-03-19T15:02:54.107Z","comments":true,"path":"2025/03/05/sam2gradio/","permalink":"https://loveaiblog.github.io/2025/03/05/sam2gradio/","excerpt":"","text":"This blog explains a simple implementation of a Gradio-based UI for the SAM2 (Segment Anything 2) video segmentation model. The code processes input videos, performs object segmentation using SAM2, and visualizes results. Let's break down the key components: Imports &amp; Device Configuration 12345678910import gradio as grimport cv2, os, numpy as npimport torch# ... other imports ...# Device selectionif torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;)else: device = torch.device(&quot;cpu&quot;) Functionality: Imports necessary libraries for computer vision, deep learning, and UI creation Automatically selects the best available computation device (CUDA GPU &gt; CPU) Video Processing Utilities 1234567def video2images(video, output_folder): # Video to frame extraction logic # ... (skipped for brevity) ...def images2video(image_folder, output_video_path, fps=24): # Frame to video conversion logic # ... (skipped for brevity) ... Key Features: video2images(): Extracts frames from video, skips first frame (assumed to be overlay grid) images2video(): Reconstructs video from processed frames with specified FPS SAM2 Inference Pipeline Let's take a closer look at the core SAM2 inference implementation that powers the video segmentation. This section contains the most critical logic for processing user inputs and generating segmentation masks. Model Initialization 12345678910111213141516171819def sam2_inference(images_path, point_x, point_y, save_dir_name): # Model configuration model_cfg = &quot;configs/sam2.1/sam2.1_hiera_l.yaml&quot; sam2_checkpoint = &quot;/path/to/checkpoint.pt&quot; # Handle different compute architectures if device.type == &quot;cuda&quot;: torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16).__enter__() if torch.cuda.get_device_properties(0).major &gt;= 8: torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cudnn.allow_tf32 = True # Initialize predictor with proper device handling from sam2.build_sam import build_sam2_video_predictor predictor = build_sam2_video_predictor( model_cfg, sam2_checkpoint, device=device ) Key Components: Loads model configuration from YAML file Initializes SAM2 predictor with pretrained weights Prompt Processing &amp; State Initialization 1234567891011121314151617181920# Initialize inference state with first video frameinference_state = predictor.init_state(video_path=images_path)predictor.reset_state(inference_state)# Process user click coordinatesann_frame_idx = 0 # First frame indexann_obj_id = 1 # First object ID# Convert user click to model inputpoints = np.array([[int(point_x), int(point_y)]], dtype=np.float32)labels = np.array([1], np.int32) # 1 = positive prompt# Add prompts to predictor_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box( inference_state=inference_state, frame_idx=ann_frame_idx, obj_id=ann_obj_id, points=points, labels=labels,) Critical Operations: Load the images from the input video Creates initial inference state from video frames Add user click coordinates to a point prompt Temporal Mask Propagation 1234567# Perform video-level mask inferencevideo_segments = &#123;&#125;for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state): video_segments[out_frame_idx] = &#123; out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids) &#125; Core Algorithm: Iterates through video frames sequentially Mask Visualization &amp; Output 123456789101112131415161718192021def coloring_mask(mask, color): # Give a mask a color # ... (skipped for brevity) ...# Iterate output masksfor out_frame_idx in range(0, len(frame_names), vis_frame_stride): frame_idx = cv2.imread(...) frame_idx = torch.tensor(frame_idx, dtype=torch.float32).to(device) mask_idx = torch.zeros_like(frame_idx) for out_obj_id, out_mask in video_segments[out_frame_idx].items(): # Apply color blending to masks colored_mask = coloring_mask(out_mask,color[out_obj_id-1]) frame_copy = frame_idx.clone() frame_copy[mask] = colored_mask[mask] mask_idx[mask] = 255 alpha = 0.7 beta = 1 - alpha frame_idx = torch.add(frame_copy * alpha, frame_idx * beta) # Save to path frame_idx = cv2.imwrite(..., frame_idx.cpu().numpy()) mask_idx = cv2.imwrite(..., mask_idx.cpu().numpy()) Visualization Techniques: Implements blending (70% mask opacity &amp; 30% frame opacity) Saves both blended visualizations and raw masks Main Function sam2_video() 123456789101112131415161718def sam2_video(video, point_x, point_y): # Convert video to images video2image_saved = ... output_video_folder = ... fps, video_name = video2images(video, video2image_saved) images_path = os.path.join(video2image_saved, video_name) # Perform SAM2 inference images_outputs_path, mask_dir = sam2_inference(images_path, height, width, video_name) # Convert images back to video output_video_path = os.path.join(output_video_folder, video_name + &#x27;.mp4&#x27;) output_mask_path = os.path.join(output_video_folder, video_name + &#x27;_mask.mp4&#x27;) images2video(images_outputs_path, output_video_path, fps) images2video(mask_dir, output_mask_path, fps) return output_video_path, output_mask_path Explanation: It first converts the video into individual images. Then, it runs SAM2 segmentation on those images. Finally, it converts the processed frames back into a video, both with and without masks, and returns the video paths for download. Gradio Interface 12345678910111213141516171819202122if __name__ == &quot;__main__&quot;: with gr.Blocks() as demo: with gr.Row(): input_video = gr.Video(format=&#x27;mp4&#x27;,label=&#x27;Source Video&#x27;) first_img = gr.Image(label=&quot;First Image&quot;) with gr.Row(): input_x_cord = gr.Textbox(label=&quot;X Cord&quot;) input_y_cord = gr.Textbox(label=&quot;Y Cord&quot;) with gr.Row(): text_button = gr.Button(&quot;Submit&quot;) with gr.Row(): output_video = gr.Video(format=&#x27;mp4&#x27;,label=&quot;SAM2 Vis&quot;,show_download_button=True) output_mask = gr.Video(format=&#x27;mp4&#x27;,label=&quot;Mask&quot;,show_download_button=True) input_video.upload(load_first_img, [input_video],[first_img]) first_img.select(get_select_coords, [first_img,input_video], [input_x_cord, input_y_cord]) text_button.click(sam2_video, [input_video, input_x_cord, input_y_cord], [output_video, output_mask]) demo.launch() Follow-up Development This blog demonstrates the implementation of SAM2 with a single point prompt. You can extend it to multiple prompts as needed (by utilizing gr.render to input a custom number of prompts). In addition, event listeners can be implemented through gr.State that enable to control the frames displayed on the UI through buttons to facilitate the selection of prompts on multiple frames. A more practical and stable implementation of SAM2 involves using a mask as the prompt. To achieve this you need to use predictor.predict() to generate a mask from a point or a box prompt using SAM (see SAM2 Official Tutorial: image_predictor_example.ipynb), which can then be used to invoke SAM2 with the mask prompt for further inference (mask prompt see SAM2 Official Tutorial: automatic_mask_generator_example.ipynb).","categories":[],"tags":[{"name":"Development","slug":"Development","permalink":"https://loveaiblog.github.io/tags/Development/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://loveaiblog.github.io/tags/Algorithm/"}]},{"title":"Convert Mask from Numpy Array to RLE (Run-Length Encoding) for CVAT","slug":"mask2rle","date":"2025-03-04T13:27:00.000Z","updated":"2025-03-18T14:05:56.162Z","comments":true,"path":"2025/03/04/mask2rle/","permalink":"https://loveaiblog.github.io/2025/03/04/mask2rle/","excerpt":"","text":"What is RLE? When reading a mask in the format of numpy.array line by line, you often encounter long sequences of consecutive 0 (or False) followed by sequences of 1 (or True). Based on this observation, we can first define a bounding box that encloses the mask. Then, we read the values line by line within this box and record the counts of consecutive values. For example, consider the following mask enclosed in a box: 12[[0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]] Reading this mask line by line, we get the sequence: 3*0, 6*1, 5*0, 7*1, 3*0. This sequence can be efficiently encoded as the RLE string: '3,6,5,7,3'. If the mask starts with a 1 instead of a 0, we prepend a 0 to the RLE string to indicate that there are zero 0s at the beginning. This is the standard rule for converting a mask to RLE format. Standard Implementation 12345678910111213141516171819202122232425def binary_image_mask_to_cvat_rle(mask: np.ndarray) -&gt; dict: # Get box information yy, xx = np.nonzero(mask) top, left = np.min(yy), np.min(xx) bottom, right = np.max(yy), np.max(xx) height, width = bottom - top + 1, right - left + 1 rle = [] offset = 0 # How many consecutive &#x27;0&#x27;s or &#x27;1&#x27;s value = 0 # Are we counting &#x27;0&#x27; or &#x27;1&#x27; # Read line by line for y in range(top, top + height): for x in range(left, left + width): if mask[y][x] == value: offset += 1 else: rle.append(offset) # Save to RLE offset = 1 # Reset &#x27;offset&#x27; value = 1 - value # Flip &#x27;value&#x27; if offset &gt; 0: rle.append(offset) return &#123;&quot;rle&quot;: rle, &quot;left&quot;: left, &quot;top&quot;: top, &quot;width&quot;: width, &quot;height&quot;: height&#125; The code requires iterating over the entire box, resulting in a loop that runs height * width times. Converting a single mask to RLE takes about 0.02 seconds on my device. When processing masks in batches, you might need to wait for tens of seconds in the CVAT interface just for format conversion. We can replace the above method with a parallelizable approach. Time-efficient Implementation First, we need to determine which positions in the mask transition from 0 to 1 and which positions transition from 1 to 0. 12345bbox_mask = mask[top:bottom + 1, left:right + 1]flat_mask = bbox_mask.flatten() # transform the mask to a 1D arraydiff = np.diff(flat_mask, prepend=0) # compute the differenceschanges = np.where(np.isin(diff, [1, -1]))[0] # find where the value changes Due to the use of prepend=0, changes=0 if flat_mask[0]=1. This is great because the first element in changes will always stands for the number of 0s in the mask. Except for the first element, which represents the count of 0s, the difference between each pair of adjacent elements in changes represents the count of consecutive 0s or 1s. The difference between the last element and the length of flat_mask is the last number to be added to the RLE. 12345rle = []rle.append(changes[0])for i in range(len(changes)-1): rle.append(changes[i+1] - changes[i])rle.append(len(flat_mask)-changes[-1]) The time-efficient implementation: 12345678910111213141516171819202122232425def binary_image_mask_to_cvat_rle(mask: np.ndarray) -&gt; dict: &quot;&quot;&quot; Optimized version of converting binary image mask to CVAT RLE. &quot;&quot;&quot; yy, xx = np.nonzero(mask) if len(yy) == 0: return &#123;&quot;size&quot;: mask.shape, &quot;counts&quot;: []&#125; top, left = np.min(yy), np.min(xx) bottom, right = np.max(yy), np.max(xx) height, width = bottom - top + 1, right - left + 1 bbox_mask = mask[top:bottom + 1, left:right + 1] flat_mask = bbox_mask.flatten() diff = np.diff(flat_mask, prepend=0) changes = np.where(np.isin(diff, [1, -1]))[0] rle = [] rle.append(changes[0]) for i in range(len(changes)-1): rle.append(changes[i+1] - changes[i]) rle.append(len(flat_mask)-changes[-1]) return &#123;&quot;rle&quot;: rle, &quot;left&quot;: left, &quot;top&quot;: top, &quot;width&quot;: width, &quot;height&quot;: height&#125;","categories":[],"tags":[{"name":"Development","slug":"Development","permalink":"https://loveaiblog.github.io/tags/Development/"}]},{"title":"Designing UI with Gradio","slug":"Designing-UI-with-Gradio","date":"2025-03-02T11:18:00.000Z","updated":"2025-03-18T14:05:42.145Z","comments":true,"path":"2025/03/02/Designing-UI-with-Gradio/","permalink":"https://loveaiblog.github.io/2025/03/02/Designing-UI-with-Gradio/","excerpt":"","text":"Quick Start with gr.Interface Let's start from a simplest demo. 1234567import gradio as grdef your_function(input_text): return &quot;For example, a translation model&#x27;s output taking&quot; + input_text + &quot;as input&quot;demo = gr.Interface(fn=your_function, inputs=&quot;text&quot;, outputs=&quot;text&quot;)demo.launch() gr.Interface allows us to define a UI for your_function. The type of inputs and outputs must be predefined. Gradio allows 'text' 'image' 'number' 'audio' 'video' for both inputs and outputs. Other types that might be useful: Input-only: 'slider' - inputting values within a specified range using a slider 'checkbox' - boolean input 'radio' - single selection input 'dropdown' - selecting from a dropdown menu Output-only: 'label' - displaying labels or classification results 'plot' - displaying charts (e.g., Matplotlib or Plotly plots) Multiple inputs and outputs by putting types into a list, for example: 1234567import gradio as grdef your_function(input_text, language): return &quot;A&quot; + language + &quot;translation model&#x27;s output taking&quot; + input_text + &quot;as input&quot;demo = gr.Interface(fn=your_function, inputs=[&quot;text&quot;,&quot;text&quot;], outputs=&quot;text&quot;)demo.launch() After running the python script, we can Ctrl + Click http://localhost:7860 from your terminal, then you can see a interface in our browser like this: Now we have everything for building a clear-enough UI for any target function. Let me give some example: 1234gr.Interface(fn=your_function, inputs=&quot;image&quot;, outputs=&quot;label&quot;) # A classification modelgr.Interface(fn=your_function, inputs=&quot;text&quot;, outputs=&quot;image&quot;) # A Text-to-Image generation modelgr.Interface(fn=your_function, inputs=[&quot;slider&quot;,&quot;slider&quot;,&quot;image&quot;], outputs=&quot;image&quot;) # Segment Anything (SAM) with point promptgr.Interface(fn=your_function, inputs=[&quot;image&quot;,&quot;video&quot;], outputs=&quot;video&quot;) # SAM2 with mask prompt The only problem now is layouts, right? When the UI is complex, not having layouts can be uncomfortable. In fact, there are many other functions that cannot be implemented with gr.Interface, such as getting information about mouse clicks, triggering inference in a way other than a button, and generating a different number of output boxes depending on the input. When you find that the simple Interface function no longer meets your needs, you may want to try gr.Blocks. Building Better Interfaces with gr.Blocks One of the most noticeable changes when using gr.Blocks is the ability to design the layout. We can now have more control over how the UI is arranged. Let's first implement the example we just saw with gr.Interface using gr.Blocks. A Simple Gradio Demo with gr.Blocks Here is a simple demo includes a text input, an output box, a submit button, and a clear button. 1234567891011121314151617import gradio as grdef your_function(name): return &quot;Hello &quot; + name + &quot;!&quot;def clear_all(): return [&quot;&quot;, &quot;&quot;]with gr.Blocks() as demo: name = gr.Textbox(label=&quot;Name&quot;) output = gr.Textbox(label=&quot;Output Box&quot;) submit_btn = gr.Button(&quot;Submit&quot;) clear_btn = gr.Button(&quot;Clear&quot;) submit_btn.click(fn=your_function, inputs=name, outputs=output) clear_btn.click(fn=clear_all, inputs=None, outputs=[name, output])demo.launch() The UI looks like this: Does the layout look a bit different? Well, the basic functionality is the same as in the previous example. In this case, we define a gr.Blocks, which is like taking a sheet of A4 paper when you start working on an assignment. Then we create two gr.Textbox and two gr.Button on the paper, and they are automatically arranged within this block area. Layout You can design your layout using gr.Column and gr.Row to make it look like the UIs you see on Hugging Face. For example: 12345678910111213141516171819import gradio as grwith gr.Blocks() as demo: with gr.Row(): text1 = gr.Textbox(label=&quot;t1&quot;) slider2 = gr.Textbox(label=&quot;s2&quot;) drop3 = gr.Dropdown([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], label=&quot;d3&quot;) with gr.Row(): with gr.Column(scale=1, min_width=300): text1 = gr.Textbox(label=&quot;prompt 1&quot;) text2 = gr.Textbox(label=&quot;prompt 2&quot;) inbtw = gr.Button(&quot;Between&quot;) text4 = gr.Textbox(label=&quot;prompt 1&quot;) text5 = gr.Textbox(label=&quot;prompt 2&quot;) with gr.Column(scale=2, min_width=300): img1 = gr.Image(&quot;images/cheetah.jpg&quot;) btn = gr.Button(&quot;Go&quot;)demo.launch() Like the following code, you can divide gr.Blocks into multiple horizontal sub-areas, and then further divide one of these sub-areas into multiple vertical sub-areas. 12345678with gr.Blocks() as demo: with gr.Row(): ... with gr.Row(): with gr.Column(): ... with gr.Column(): ... To limit the shape of a block, try: 1with gr.Column(scale=2, min_width=300) Function Trigger Methods Although my favorite is Button, there are other ways to trigger your function. You can try using an event listener or triggering the function by pressing ENTER. The former can monitor changes in the input box in real-time, while the latter replaces the button with a keyboard key. 1234567891011# Button btn = gr.Button(&quot;Submit&quot;)submit_btn.click(fn=your_function, inputs=input, outputs=output) # Event listener inp = gr.Textbox()inp.change(welcome, inp, out)# &#x27;ENTER&#x27; inp = gr.Textbox(label=&quot;Input&quot;)inp.submit(greet, name, [output, trigger]) You can also set multiple triggers using gr.Blocks(). 1gr.on(triggers=[name.submit, greet_btn.click], fn=greet, inputs=name, outputs=output).then(clear_name, outputs=[name]) You may use .then(clear_name, outputs=[name]) to call a second function after 'greet' has been triggered. Here, outputs=[name] means that the return value will update the \"name\" input box. The .then() operation can actually be written inside greet, making greet look cleaner, but it is not mandatory. Controling the visibility of a sub-block 123456789101112131415161718192021222324252627import gradio as grwith gr.Blocks() as demo: name_box = gr.Textbox(label=&quot;Name&quot;) age_box = gr.Number(label=&quot;Age&quot;, minimum=0, maximum=100) symptoms_box = gr.CheckboxGroup([&quot;Cough&quot;, &quot;Fever&quot;, &quot;Runny Nose&quot;]) submit_btn = gr.Button(&quot;Submit&quot;) with gr.Column(visible=False) as output_col: diagnosis_box = gr.Textbox(label=&quot;Diagnosis&quot;) patient_summary_box = gr.Textbox(label=&quot;Patient Summary&quot;) def submit(name, age, symptoms): return &#123; submit_btn: gr.Button(visible=False), output_col: gr.Column(visible=True), diagnosis_box: &quot;covid&quot; if &quot;Cough&quot; in symptoms else &quot;flu&quot;, patient_summary_box: f&quot;&#123;name&#125;, &#123;age&#125; y/o&quot;, &#125; submit_btn.click( submit, [name_box, age_box, symptoms_box], [submit_btn, diagnosis_box, patient_summary_box, output_col], )demo.launch() In this example, clicking the submit_btn triggers the submit function. The change in visibility is specified as one of the outputs. You can modify the visibility of individual components or an entire block. If it applies to a block, you need to define a variable for it when creating the block. Determine the Output Boxes Based on Input If you are unsure how many outputs will be generated before entering your inputs, you can use gr.render(). 1234567891011121314import gradio as grwith gr.Blocks() as demo: input_text = gr.Textbox(label=&quot;Input&quot;) @gr.render(inputs=input_text) def show_split(text): if len(text) == 0: gr.Markdown(&quot;## No Input Provided&quot;) else: for letter in text: gr.Textbox(letter)demo.launch() @gr.render(inputs=input_text) is actually listening to the change in input_text if there is no trigger defined. You can use this listening mechanism to create some interesting designs. For example: 1234567891011121314151617181920212223import gradio as grwith gr.Blocks() as demo: text_count = gr.State(1) add_btn = gr.Button(&quot;Add Box&quot;) add_btn.click(lambda x: x + 1, text_count, text_count) @gr.render(inputs=text_count) def render_count(count): boxes = [] for i in range(count): box = gr.Textbox(key=i, label=f&quot;Box &#123;i&#125;&quot;) boxes.append(box) def merge(*args): return &quot; &quot;.join(args) merge_btn.click(merge, boxes, output) merge_btn = gr.Button(&quot;Merge&quot;) output = gr.Textbox(label=&quot;Merged Output&quot;)demo.launch() Here, gr.State() allows you to listen to events in the state variable. If you are adding a new box through gr.render() but have already input something into the existing boxes, remember to set key= to maintain the content in the displayed boxes. Provide Some Examples on the UI If you want to show users how to use your UI, you can use gr.Examples() to provide pre-filled example inputs. This helps users understand how to interact with your interface more effectively. 123456789101112131415161718192021222324252627282930313233343536import gradio as grdef calculator(num1, operation, num2): if operation == &quot;add&quot;: return num1 + num2 elif operation == &quot;subtract&quot;: return num1 - num2 elif operation == &quot;multiply&quot;: return num1 * num2 elif operation == &quot;divide&quot;: return num1 / num2with gr.Blocks() as demo: with gr.Row(): with gr.Column(): num_1 = gr.Number(value=4, label=&quot;Number 1&quot;) operation = gr.Radio([&quot;add&quot;, &quot;subtract&quot;, &quot;multiply&quot;, &quot;divide&quot;], label=&quot;Operation&quot;) num_2 = gr.Number(value=0, label=&quot;Number 2&quot;) submit_btn = gr.Button(value=&quot;Calculate&quot;) with gr.Column(): result = gr.Number(label=&quot;Result&quot;) submit_btn.click( calculator, inputs=[num_1, operation, num_2], outputs=[result], api_name=False ) examples = gr.Examples( examples=[ [5, &quot;add&quot;, 3], [4, &quot;divide&quot;, 2], [-4, &quot;multiply&quot;, 2.5], [0, &quot;subtract&quot;, 1.2], ], inputs=[num_1, operation, num_2], label=&quot;Examples&quot; )demo.launch(show_api=False) In this example, gr.Examples() provides a set of predefined input combinations. Users can click on any example to automatically fill the input boxes with those values. This feature makes it easier for users to test different scenarios and understand how the calculator works. Select a Point on the gr.Image or gr.Dataframe If you want to click on an image to get the coordinates of where you clicked relative to the image, you can use gr.select() to achieve this. 12345678910def get_select_coords(img, evt: gr.SelectData): cord_x = evt.index[0] cord_y = evt.index[1] return cord_x, cord_ywith gr.Blocks() as demo: first_img = gr.Image(label=&quot;First Image&quot;) x_cord = gr.Textbox(label=&quot;X Coordinate&quot;) y_cord = gr.Textbox(label=&quot;Y Coordinate&quot;) first_img.select(get_select_coords, [first_img], [x_cord, y_cord]) Similarly, gr.Dataframe is a table-format component that also supports the select method. Summary of Useful Functions for gr.Blocks 12345678gr.Row(scale=1, min_width=300) gr.Column() gr.Column(visible=False) # Hide a block unless a trigger change the state of &#x27;visible&#x27; gr.select() # Returns the interaction information when interacting with a variable of a certain type gr.Tab((&quot;Next Page&quot;)) # Select one of the multiple Tabs that the current UI displays. gr.Accordion(&quot;Open for More!&quot;, open=False) # A block that can be hidden gr.render() # Generate boxes according to your inputs gr.Examples() # Display input examples, support clicking examples to fill in the input","categories":[],"tags":[{"name":"Development","slug":"Development","permalink":"https://loveaiblog.github.io/tags/Development/"}]}],"categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://loveaiblog.github.io/tags/Algorithm/"},{"name":"Development","slug":"Development","permalink":"https://loveaiblog.github.io/tags/Development/"}]}