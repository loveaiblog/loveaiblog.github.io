<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>How the Segment Anything Model 2 (SAM2) Works at a High Level | Zihan&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="Algorithm" />
  
  
  
  
  <meta property="og:type" content="article">
<meta property="og:title" content="How the Segment Anything Model 2 (SAM2) Works at a High Level">
<meta property="og:url" content="https://loveaiblog.github.io/2025/03/18/sam2/index.html">
<meta property="og:site_name" content="Zihan&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://loveaiblog.github.io/2025/03/18/sam2/image1.PNG">
<meta property="og:image" content="https://loveaiblog.github.io/2025/03/18/sam2/image2.PNG">
<meta property="og:image" content="https://loveaiblog.github.io/2025/03/18/sam2/image3.PNG">
<meta property="article:published_time" content="2025-03-18T14:02:01.000Z">
<meta property="article:modified_time" content="2025-03-18T14:21:24.541Z">
<meta property="article:author" content="Zihan Liu">
<meta property="article:tag" content="Algorithm">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://loveaiblog.github.io/2025/03/18/sam2/image1.PNG">
  
    <link rel="alternate" href="/atom.xml" title="Zihan&#39;s Blog" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">

  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Oswald%3A300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/fashion.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >

<meta name="generator" content="Hexo 7.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  


<header id="allheader" class="site-header" role="banner" 
   >
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="Zihan&#39;s Blog" rel="home"> Zihan&#39;s Blog </a>
            
          </h1>
          
          
            <div class="site-description">A diary of AI development and learning activities.</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>

            <div class="clearfix sf-menu">
              <ul id="main-nav" class="menu sf-js-enabled sf-arrows"  style="touch-action: pan-y;">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663" linktext="/"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663" linktext="archives"> <a class="" href="/archives">Archives</a> </li>
                    
              </ul>
            </div>
          </nav>

      </div>
  </div>
</header>


  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-sam2" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      How the Segment Anything Model 2 (SAM2) Works at a High Level
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2025/03/18/sam2/" class="article-date">
	  <time datetime="2025-03-18T14:02:01.000Z" itemprop="datePublished">March 18, 2025</time>
	</a>

       
      
	<span id="busuanzi_container_page_pv">
	  Total Views for This Post: <span id="busuanzi_value_page_pv"></span>
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>With the continued development of <a target="_blank" rel="noopener" href="https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/">Segment Anything Model 2 (SAM2)</a>, I believe it’s essential to dive into the algorithmic details of SAM2 to gain a deeper understanding of how it works and its potential. In this blog, I’ll highlight the most important aspects of SAM2, distilling key insights to help readers grasp its fundamentals and applications.</p>
<img src="image1.PNG">

<h2 id="How-to-use-SAM2"><a href="#How-to-use-SAM2" class="headerlink" title="How to use SAM2"></a>How to use SAM2</h2><p>The primary function of SAM2 is to extract masklets (i.e., a masklet is a set of masks for the same object) from videos based on interactive prompts. The supported prompt formats include:  </p>
<ul>
<li><strong>Point</strong>: Formatted as <code>[x, y]</code>, with attributes for <code>positive</code> and <code>negative</code>.  </li>
<li><strong>Box</strong>: Formatted as <code>[x_min, y_min, x_max, y_max]</code> without normalization.  </li>
<li><strong>Mask</strong>: Formatted as <code>numpy.array(H, W)</code>.</li>
</ul>
<p>The model takes in all frames of a video, the content of the prompt(s), the corresponding frame, and the instance ID as input. It then outputs a dictionary of masks indexed by frame number and prompt instance ID, formatted as:  </p>
<pre><code class="language-python">&#123;
  0: &#123;&#39;obj_0&#39;:..., &#39;obj_1&#39;:...&#125;,  
  1: &#123;&#39;obj_0&#39;:..., &#39;obj_1&#39;:...&#125;,  
  ...
&#125;
</code></pre>
<p>According to the official documentation, it may lost tracking during inference, as shown in Step 1, Frames 3 &amp; 4 of the figure. However, thanks to the memory bank, a simple positive point can restore tracking (as demonstrated in Step 2).  </p>
<p>In practice, this feature isn’t particularly useful since it provides only a limited efficiency boost for point-based pipelines while being quite complex to deploy. If deploying on an annotation platform, the best approach is to generate masks using SAM and then process them with SAM2. For developing automated annotation tools, <strong>bounding boxes</strong> are the most recommended prompt format.</p>
<img src="image2.PNG">

<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>According to the official report, SAM2’s multi-object inference is essentially built upon single-frame inference. In other words, during the model’s forward pass, it only considers <strong>one</strong> prompt at a time. For example, if a prompt marks a cat in the first frame, SAM2 follows these steps for subsequent inference:  </p>
<ul>
<li><strong>Image Encoder</strong>: Uses <strong>MAE-pretrained Hiera</strong> (a pyramid feature network) as the image encoder to extract visual information.  </li>
<li><strong>Memory Attention</strong>: Employs a Transformer-based structure to integrate image features with the memory bank. Specifically, it stacks multiple layers of <strong>self-attention</strong> (for image embeddings) and <strong>cross-attention</strong> in an interleaved “sandwich” pattern.</li>
<li><strong>Prompt Encoder</strong>: Extracts prompt information in token form, using the same prompt encoder as SAM.  </li>
<li><strong>Memory Decoder</strong>: Performs attention operations on the fused image embeddings, memory-merged outputs, and prompt tokens (see diagram for details).  <ul>
<li>The <strong>IoU score</strong> selects the highest-scoring mask from multiple outputs.  </li>
<li>The <strong>occlusion score</strong> determines whether the target object is absent in the current frame (which explains why SAM2 sometimes loses tracking).  </li>
<li>The <strong>object pointer</strong> carries memory information forward for the next frame’s prediction.</li>
</ul>
</li>
<li><strong>Memory Encoder</strong>: The output masks are downsampled, element-wise summed with the image encoder output, then passed through a <strong>convolutional</strong> network before being stored in the memory bank.  </li>
<li><strong>Memory Bank</strong>: Stores information from the most recent <strong>N</strong> frames and permanently retains <strong>M</strong> frames that contain prompts.</li>
</ul>
<img src="image3.PNG">

<h2 id="Data-and-Training"><a href="#Data-and-Training" class="headerlink" title="Data and Training"></a>Data and Training</h2><p>The development team collected and annotated the <strong>SA-V dataset</strong>, which consists of <strong>50.9K videos</strong> and <strong>190.9K masklets</strong>. This dataset was manually labeled using both <strong>SAM</strong> and <strong>SAM2</strong>. As the dataset size increased, SAM2’s performance improved, and annotation efficiency also benefited from iterative enhancements to SAM2 itself.  </p>
<p>SAM2 was initially trained on <strong>SA-1B</strong>, a static image segmentation dataset. Given its architecture, during this phase, <strong>memory attention</strong> only applied <strong>self-attention</strong> to image embeddings, while <strong>output tokens</strong> were ignored in the memory decoder. The loss function used in pretraining differs slightly from that of SAM—details can be found in the research paper.  </p>
<p>After pretraining, SAM2 was further trained on <strong>video data</strong>, which included <strong>SA-V</strong>, <strong>internal company-licensed videos</strong>, and <strong>a subset (10%) of SA-1B</strong>. The training alternated between video and static image datasets, with the data volume for each training iteration proportional to the dataset’s size.  </p>
<p>A few additional key points:  </p>
<ul>
<li>During training, the ratio of prompt types was <strong>50% mask, 25% point, and 25% box</strong>.  </li>
<li>Videos had a <strong>50% probability of being flipped</strong> to enable <strong>bidirectional propagation</strong> (which I discuss in another blog post).  </li>
<li><strong>Random horizontal flips</strong> and <strong>random affine transforms</strong> were used for data augmentation.</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>SAM2 represents a significant step forward in video segmentation by integrating memory-based tracking with prompt-guided mask extraction. While it builds on the strengths of SAM, its reliance on single-frame inference and complex deployment considerations mean it is best suited for specific use cases, such as annotation platforms and automated labeling tools. As research in segmentation models continues to evolve, improvements in tracking stability and efficiency will likely make models like SAM2 even more powerful.  </p>
<p>I hope this post helps clarify how SAM2 works at a high level. If you’re interested in exploring more, I encourage you to experiment with SAM2 yourself or dive into the official research paper for deeper insights.</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm/" rel="tag">Algorithm</a></li></ul>

      
        
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'loveaiblog';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/03/25/groundingdino/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Why Can Grounding DINO Achieve Zero-Shot Object Detection?
        
      </div>
    </a>
  
  
    <a href="/2025/03/13/auto-video-tracking/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Autonomous Video Tracking &amp; Segmentation</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
      <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-use-SAM2"><span class="nav-number">1.</span> <span class="nav-text">How to use SAM2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture"><span class="nav-number">2.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-and-Training"><span class="nav-number">3.</span> <span class="nav-text">Data and Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">4.</span> <span class="nav-text">Conclusion</span></a></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav> -->
    <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2025 Zihan&#39;s Blog All Rights Reserved.
        
            <span id="busuanzi_container_site_uv">
              Total Visitors: <span id="busuanzi_value_site_uv"></span> 
              Total Views: <span id="busuanzi_value_site_pv"></span>
            </span>
          
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");

    wrapdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";


    <!-- headerblur min height -->
    
    
</script>
    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


<script src="/js/bootstrap.js"></script>


<script src="/js/main.js"></script>








  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
